{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfd9542d",
      "metadata": {
        "id": "bfd9542d"
      },
      "source": [
        "\n",
        "# 🧠 Heuristic-Based - Annotating Human vs. AI Content\n",
        "\n",
        "This notebook helps you compare AI-generated and human-edited text using:\n",
        "- **Levenshtein distance** (for textual edits)\n",
        "- **Cosine similarity** (from transformer embeddings)\n",
        "\n",
        "We will also show how to build and deploy it to a **Streamlit app** and then to Streamlit Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Levenshtein Distance (Edit Distance)**\n",
        "\n",
        "This is a way of measuring how different two pieces of text are by counting the minimum number of edits needed to change one into the other.\n",
        "\n",
        "**The allowed edits are:**\n",
        "\n",
        "Insertions (adding a character)\n",
        "\n",
        "Deletions (removing a character)\n",
        "\n",
        "Substitutions (replacing one character with another)\n",
        "\n",
        "**Example:**\n",
        "\"cat\" → \"cut\" requires 1 substitution (a → u), so the Levenshtein distance = 1.\n",
        "\n",
        "So, the smaller the distance, the more similar the texts; a larger distance means they are more divergent.\n",
        "\n"
      ],
      "metadata": {
        "id": "4UHg47DsOslf"
      },
      "id": "4UHg47DsOslf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine Similarity (using embeddings from transformers)**\n",
        "When comparing meaning between long texts or sentences, we often convert the texts into high-dimensional number vectors using transformer models like BERT. These vectors capture the meaning/context of the text — these are called embeddings.\n",
        "\n",
        "Cosine similarity measures how close the direction of two embedding vectors are, regardless of their length.\n",
        "\n",
        "The value ranges from –1 to 1\n",
        "\n",
        "1 → very similar / pointing in the same direction\n",
        "\n",
        "0 → no relation\n",
        "\n",
        "–1 → completely opposite meanings (rare in practice)\n",
        "\n",
        "Think of two arrows in space: the more they point in the same direction, the higher their cosine similarity."
      ],
      "metadata": {
        "id": "rdSF-qL9kasI"
      },
      "id": "rdSF-qL9kasI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139aa292",
      "metadata": {
        "id": "139aa292"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.40.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance torch"
      ],
      "metadata": {
        "id": "FdDlqqhsTFtc"
      },
      "id": "FdDlqqhsTFtc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textdistance\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "te7Dp-Tki3V7"
      },
      "id": "te7Dp-Tki3V7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the Cosine Similarity between AI and a human-written texts\n",
        "ai_text = \"Artificial intelligence is transforming industries by automating complex tasks.\"\n",
        "human_text = \"AI is rapidly changing industries by automating both simple and complex operations.\""
      ],
      "metadata": {
        "id": "SiuAyyAXi67U"
      },
      "id": "SiuAyyAXi67U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 1: Levenshtein Distance ---\n",
        "# Calculate normalized Levenshtein distance between the two strings.\n",
        "#   - Levenshtein distance counts how many edits (insertions, deletions, substitutions)\n",
        "#     are needed to turn one string into the other.\n",
        "#   - 'normalized_distance()' scales that value between 0 and 1\n",
        "#       -> 0 = the texts are identical\n",
        "#       -> 1 = completely different\n",
        "lev_distance = textdistance.levenshtein.normalized_distance(ai_text, human_text)\n",
        "print(f\"Normalized Levenshtein Distance: {lev_distance:.3f} → This means the texts are about {lev_distance*100:.1f}% different (and roughly {100 - lev_distance*100:.1f}% similar)\")"
      ],
      "metadata": {
        "id": "RNuPeBEgjLTt"
      },
      "id": "RNuPeBEgjLTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw Levenshtein distance (minimum number of edits required to turn one string into another)\n",
        "raw_dist = textdistance.levenshtein.distance(ai_text, human_text)\n",
        "print(f\"Raw Levenshtein Distance: {raw_dist} → This means {raw_dist} character-level edits are needed to turn one text into the other.\")"
      ],
      "metadata": {
        "id": "LdoYbf3zlJfP"
      },
      "id": "LdoYbf3zlJfP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 2: Embedding Cosine Similarity using Transformers ---\n",
        "\n",
        "# Specify the model to use for generating sentence embeddings.\n",
        "# 'all-MiniLM-L6-v2' is a small and fast model from the Sentence-Transformers family,\n",
        "# optimized for computing sentence-level semantic similarity.\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Load the tokenizer associated with the model.\n",
        "# The tokenizer converts raw text into tokens and input IDs the model can process.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the actual transformer model.\n",
        "# This model will generate context-aware embeddings (vectors) for the input text.\n",
        "model = AutoModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "a69FAScmrCNZ"
      },
      "id": "a69FAScmrCNZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get the sentence embedding for any given text\n",
        "def get_embedding(text):\n",
        "    # Step 1: Tokenize the input text\n",
        "    # - return_tensors=\"pt\": returns PyTorch tensors\n",
        "    # - truncation=True: cuts off text that's too long\n",
        "    # - padding=True: ensures all sequences are the same length\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Step 2: Disable gradient tracking since we're not training (just getting embeddings)\n",
        "    with torch.no_grad():\n",
        "        # Step 3: Pass the tokenized input through the model\n",
        "        output = model(**inputs)\n",
        "\n",
        "    # Step 4: Take the mean of the last hidden state across tokens (dim=1)\n",
        "    # - This gives a single 768-dimensional vector representing the sentence\n",
        "    return output.last_hidden_state.mean(dim=1)"
      ],
      "metadata": {
        "id": "WqH3BEu_rrKV"
      },
      "id": "WqH3BEu_rrKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the sentence embedding for the AI-generated text\n",
        "# This will be a 768-dimensional vector that captures the overall meaning of the sentence\n",
        "embedding_ai = get_embedding(ai_text)\n",
        "\n",
        "# Generate the sentence embedding for the human-written text\n",
        "# Also returns a 768-dimensional vector representing the sentence's semantic meaning\n",
        "embedding_human = get_embedding(human_text)"
      ],
      "metadata": {
        "id": "UIsLqlO2t4UI"
      },
      "id": "UIsLqlO2t4UI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_ai"
      ],
      "metadata": {
        "id": "gc7uHfpi1kww"
      },
      "id": "gc7uHfpi1kww",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_human"
      ],
      "metadata": {
        "id": "_yX7zbGI1ttz"
      },
      "id": "_yX7zbGI1ttz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the cosine similarity between the two sentence embeddings\n",
        "# - Cosine similarity measures how close the two vectors point in direction\n",
        "# - Result ranges from -1 (opposite) to 1 (identical); 0 means unrelated\n",
        "cos_sim = torch.nn.functional.cosine_similarity(embedding_ai, embedding_human).item()\n",
        "\n",
        "# Print the similarity score, rounded to 3 decimal places\n",
        "print(f\"Cosine Similarity: {cos_sim:.3f}\")\n",
        "\n",
        "#1.0 → vectors point in the same direction (very similar)\n",
        "\n",
        "#0.0 → vectors are 90° apart (not related)\n",
        "\n",
        "#–1.0 → vectors point in opposite directions (very different)"
      ],
      "metadata": {
        "id": "IH35lPgZ28Z8"
      },
      "id": "IH35lPgZ28Z8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cosine similarity score of 0.861 means the two sentence embeddings are:\n",
        "\n",
        "✅ Highly similar in meaning — but not identical."
      ],
      "metadata": {
        "id": "5w55scJr3fSt"
      },
      "id": "5w55scJr3fSt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f49de0a",
      "metadata": {
        "id": "4f49de0a"
      },
      "outputs": [],
      "source": [
        "# --- Attribution Confidence Leaderboard ---\n",
        "\n",
        "# Combine normalized Levenshtein distance and cosine similarity into a weighted score for each category\n",
        "\n",
        "# Score weights: you can tweak these based on importance\n",
        "weight_lev = 0.4   # weight for character-level difference (1 - lev_distance)\n",
        "weight_cos = 0.6   # weight for semantic similarity\n",
        "\n",
        "# Invert Levenshtein distance (because lower distance = more similar)\n",
        "lev_sim = 1 - lev_distance\n",
        "\n",
        "# Compute a combined confidence score for each category\n",
        "score_ai_generated = (lev_sim * weight_lev) + (cos_sim * weight_cos)\n",
        "score_human_ai_mix = (lev_sim * 0.5) + (cos_sim * 0.5)\n",
        "score_human_from_seed = (lev_sim * 0.2) + (cos_sim * 0.8)\n",
        "\n",
        "# Put scores into a dictionary\n",
        "scores = {\n",
        "    \"AI-Generated\": score_ai_generated,\n",
        "    \"Human-AI Co-Creation\": score_human_ai_mix,\n",
        "    \"Human-Written (Assumed no AI applied))\": score_human_from_seed\n",
        "}\n",
        "\n",
        "# Sort by highest score\n",
        "sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print leaderboard\n",
        "print(\"\\n🔍 Attribution Leaderboard:\")\n",
        "for label, score in sorted_scores:\n",
        "    print(f\" - {label:<30} → Confidence Score: {score:.2f}\")\n",
        "\n",
        "# Final decision: the label with the highest score\n",
        "top_label, top_score = sorted_scores[0]\n",
        "print(f\"\\n✅ Final Attribution: {top_label} (Confidence: {top_score:.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Plot Confidence Score Graph\n",
        "labels = list(scores.keys())\n",
        "values = list(scores.values())\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(labels, values)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Confidence Score')\n",
        "plt.title('Attribution Confidence Leaderboard')\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "# Annotate bars with score values\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0GHPkEuntmQ-"
      },
      "id": "0GHPkEuntmQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚠️ **CAUTION: Heuristic-Based Attribution**\n",
        "\n",
        "This method is **simple**, **transparent**, and useful for quick experimentation. However, it comes with notable limitations:\n",
        "\n",
        "- ❌ **Not trained on labeled data** — relies on hand-crafted logic  \n",
        "- 📏 **Lacks precision at scale** — not suitable for production environments  \n",
        "- 🔄 **Easily fooled by paraphrasing or slight rewording**  \n",
        "- 🧪 Performs best for **exploratory analysis**, **rapid prototyping**, or **educational demonstrations**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **When It’s Useful**\n",
        "\n",
        "- Teaching or learning the **basics of content attribution**  \n",
        "- Rapidly exploring **style differences** between texts  \n",
        "- Building a **lightweight prototype** before investing in more complex models  \n",
        "- Supplementing model outputs with **rule-based insights**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Suggested Best Practices**\n",
        "\n",
        "- Use as a **baseline** to compare with ML or zero-shot methods  \n",
        "- Combine with **metadata** (e.g., time of writing, writing tools used) to improve accuracy  \n",
        "- Apply in **controlled environments** where risk of misclassification is low  \n",
        "- Always pair with **human judgment** in sensitive or impactful use cases\n"
      ],
      "metadata": {
        "id": "PAjlapAiBrxO"
      },
      "id": "PAjlapAiBrxO"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}